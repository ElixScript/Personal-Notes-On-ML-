{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1522c5-6e31-41de-9475-e617898234ee",
   "metadata": {},
   "source": [
    "# Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625dceb9-d8c6-4301-a461-1569885f1a82",
   "metadata": {},
   "source": [
    "## importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d8d29f-bfb6-4147-8cc4-a391f920729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\acer\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\acer\\anaconda3\\lib\\site-packages (from keras) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from keras) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538686de-61bc-499c-9a93-7dcb38904dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.17.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\ACER\\anaconda3\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "---\n",
      "Name: keras\n",
      "Version: 3.5.0\n",
      "Summary: Multi-backend Keras.\n",
      "Home-page: https://github.com/keras-team/keras\n",
      "Author: Keras team\n",
      "Author-email: keras-users@googlegroups.com\n",
      "License: Apache License 2.0\n",
      "Location: C:\\Users\\ACER\\anaconda3\\Lib\\site-packages\n",
      "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
      "Required-by: tensorflow-intel\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe70927a-2740-4f5e-baa7-f7c084b77385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e73b8-bf58-426c-9cbf-5cef8f2ad215",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df256d-36b7-42fd-aabf-faf3f6d8b767",
   "metadata": {},
   "source": [
    "### preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "333e1aeb-f890-4db9-9fd0-42bd5c0965d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3418 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20c4b5-2b4c-4c74-adaf-ec35b780309f",
   "metadata": {},
   "source": [
    "### preprocessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3dba2cbc-52f9-4563-a530-e6c53c3ec052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2005 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64,64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary'\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68b369-19ff-4d04-9892-615f5fc9e7b1",
   "metadata": {},
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b4adb-9813-4edd-b19f-b128b6f4d622",
   "metadata": {},
   "source": [
    "### initialising the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae5c69d1-18e8-47f7-a481-12dfca24e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e20a26-7336-4f2d-b57a-8c71033e4bff",
   "metadata": {},
   "source": [
    "### step 1 : convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4039f56c-a5f1-4339-8362-18a7e4a4b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0176529-ad13-4446-b2c7-7ef56dcf2435",
   "metadata": {},
   "source": [
    "### step 2 : pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1616e5b8-4b2e-488d-8092-7d69144a77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7388e91-eba6-4979-bfee-41672ec0bd20",
   "metadata": {},
   "source": [
    "### adding a second convolutional layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "571016b7-8a21-4cea-af92-12c2e859b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad66d9e-eca9-4910-9219-f40751535d17",
   "metadata": {},
   "source": [
    "### step 3 : flattening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7568af7c-4f83-4dbc-9b94-b2f827994273",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c69f96-a10f-4d85-8d71-413b28241ccc",
   "metadata": {},
   "source": [
    "### step 4 : Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f6600b3-cab3-44a4-92a3-4fd7343cbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1f546-232d-4b37-bbd4-910ebd196c71",
   "metadata": {},
   "source": [
    "### step 5 : Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d9d46db-bf62-420a-adc1-988b96750808",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f5b96-d339-44fd-ae19-2417064fc666",
   "metadata": {},
   "source": [
    "## Training The CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef07a1-a14c-4c2c-837f-d53bd38899e7",
   "metadata": {},
   "source": [
    "### compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5192475e-550f-4f97-ac43-d42a4e3d5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabb943-41ee-4884-9983-21981078a111",
   "metadata": {},
   "source": [
    "### Training CNN on training set and evaluating on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "284e5065-2bbe-4d2c-81c4-cc0872f15c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - accuracy: 0.4589 - loss: -21112.3809 - val_accuracy: 0.5012 - val_loss: -696633.2500\n",
      "Epoch 2/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 153ms/step - accuracy: 0.4769 - loss: -3020098.5000 - val_accuracy: 0.5012 - val_loss: -20500110.0000\n",
      "Epoch 3/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 154ms/step - accuracy: 0.4763 - loss: -40631152.0000 - val_accuracy: 0.5012 - val_loss: -133913800.0000\n",
      "Epoch 4/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 156ms/step - accuracy: 0.4834 - loss: -205134784.0000 - val_accuracy: 0.5012 - val_loss: -478682432.0000\n",
      "Epoch 5/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 152ms/step - accuracy: 0.4940 - loss: -642353792.0000 - val_accuracy: 0.5012 - val_loss: -1236497792.0000\n",
      "Epoch 6/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - accuracy: 0.4840 - loss: -1578189952.0000 - val_accuracy: 0.5012 - val_loss: -2625547776.0000\n",
      "Epoch 7/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 155ms/step - accuracy: 0.4831 - loss: -3222524928.0000 - val_accuracy: 0.5012 - val_loss: -4907266048.0000\n",
      "Epoch 8/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 163ms/step - accuracy: 0.4988 - loss: -5691651072.0000 - val_accuracy: 0.5012 - val_loss: -8342958080.0000\n",
      "Epoch 9/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - accuracy: 0.4917 - loss: -9680990208.0000 - val_accuracy: 0.5012 - val_loss: -13237716992.0000\n",
      "Epoch 10/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 150ms/step - accuracy: 0.4946 - loss: -14865682432.0000 - val_accuracy: 0.5012 - val_loss: -19821170688.0000\n",
      "Epoch 11/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 154ms/step - accuracy: 0.4954 - loss: -22023340032.0000 - val_accuracy: 0.5012 - val_loss: -28474535936.0000\n",
      "Epoch 12/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 150ms/step - accuracy: 0.4960 - loss: -31307497472.0000 - val_accuracy: 0.5012 - val_loss: -39504617472.0000\n",
      "Epoch 13/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 145ms/step - accuracy: 0.4971 - loss: -42920493056.0000 - val_accuracy: 0.5012 - val_loss: -53210222592.0000\n",
      "Epoch 14/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 155ms/step - accuracy: 0.4804 - loss: -59289882624.0000 - val_accuracy: 0.5012 - val_loss: -69774999552.0000\n",
      "Epoch 15/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 150ms/step - accuracy: 0.4876 - loss: -76826370048.0000 - val_accuracy: 0.5012 - val_loss: -89643302912.0000\n",
      "Epoch 16/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 151ms/step - accuracy: 0.4951 - loss: -96582098944.0000 - val_accuracy: 0.5012 - val_loss: -112744742912.0000\n",
      "Epoch 17/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 156ms/step - accuracy: 0.4790 - loss: -123809030144.0000 - val_accuracy: 0.5012 - val_loss: -140124061696.0000\n",
      "Epoch 18/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - accuracy: 0.4845 - loss: -153347981312.0000 - val_accuracy: 0.5012 - val_loss: -171413782528.0000\n",
      "Epoch 19/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 153ms/step - accuracy: 0.4765 - loss: -188819472384.0000 - val_accuracy: 0.5012 - val_loss: -207027470336.0000\n",
      "Epoch 20/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 153ms/step - accuracy: 0.4905 - loss: -220068134912.0000 - val_accuracy: 0.5012 - val_loss: -247281549312.0000\n",
      "Epoch 21/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 151ms/step - accuracy: 0.4823 - loss: -267907170304.0000 - val_accuracy: 0.5012 - val_loss: -292859019264.0000\n",
      "Epoch 22/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 153ms/step - accuracy: 0.4852 - loss: -312027742208.0000 - val_accuracy: 0.5012 - val_loss: -343704633344.0000\n",
      "Epoch 23/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 156ms/step - accuracy: 0.4668 - loss: -383234703360.0000 - val_accuracy: 0.5012 - val_loss: -400242999296.0000\n",
      "Epoch 24/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 157ms/step - accuracy: 0.4888 - loss: -423069876224.0000 - val_accuracy: 0.5012 - val_loss: -462040301568.0000\n",
      "Epoch 25/25\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 157ms/step - accuracy: 0.4782 - loss: -499838550016.0000 - val_accuracy: 0.5012 - val_loss: -530002051072.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x221c66ea660>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7a61a-027f-46e2-b6ea-f0f9247ce526",
   "metadata": {},
   "source": [
    "### making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "979592a5-498f-41a8-a549-d7a7c3fabdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/test_set/cats/cat.4987.jpg', target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1 :\n",
    "    prediction = 'dog'\n",
    "else : \n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ad945161-fb2f-4eb6-a45c-da648fdab113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
